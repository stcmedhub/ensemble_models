{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image, ImageEnhance\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# TensorFlow and Keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, Callback\n",
    "from tensorflow.keras.metrics import (\n",
    "    AUC, Precision, Recall, \n",
    "    TruePositives, TrueNegatives, \n",
    "    FalsePositives, FalseNegatives\n",
    ")\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "\n",
    "# Albumentations for advanced image augmentation\n",
    "from albumentations import (\n",
    "    HueSaturationValue, RGBShift, RandomRotate90, \n",
    "    RandomFog, RandomRain, RandomSnow, Spatter, \n",
    "    GaussNoise, CLAHE, CoarseDropout, MedianBlur, PixelDropout\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Albumentations augmentations\n",
    "rotate_aug = RandomRotate90(always_apply=True, p=1.0)\n",
    "hue_sat_val = HueSaturationValue(p=1.0, sat_shift_limit=(-5, 20), val_shift_limit=(-5, 10))\n",
    "gauss_noise = GaussNoise(p=1.0, var_limit=(10.0, 100.0), per_channel=True)\n",
    "clahe = CLAHE(p=1.0, clip_limit=(1, 5), tile_grid_size=(10, 10))\n",
    "coarse_dropout = CoarseDropout(p=1.0, max_holes=30, max_height=10, max_width=10, min_holes=10, min_height=8, min_width=8)\n",
    "pixel_dropout = PixelDropout(p=1.0, dropout_prob=0.05, per_channel=True)\n",
    "splatter_configs = [\n",
    "    Spatter(p=1.0, mean=(0.46, 0.46), std=(2.3, 2.3), gauss_sigma=(0.99, 0.99), intensity=(0.2, 0.2), mode=[\"rain\"]),\n",
    "    Spatter(p=1.0, mean=(0.66, 0.66), std=(0.3, 0.3), gauss_sigma=(0.33, 0.33), intensity=(0.39, 0.39), mode=[\"rain\"]),\n",
    "    Spatter(p=1.0, mean=(0.65, 0.65), std=(0.3, 0.3), gauss_sigma=(3.16, 3.16), intensity=(0.33, 0.33), mode=[\"rain\"])\n",
    "]\n",
    "fog_configs = [\n",
    "    RandomFog(p=1.0, fog_coef_lower=0.1, fog_coef_upper=0.4, alpha_coef=0.5),\n",
    "    RandomFog(p=1.0, fog_coef_lower=0.04, fog_coef_upper=0.32, alpha_coef=0.94)\n",
    "]\n",
    "random_snow = RandomSnow(p=1.0, snow_point_lower=0.1, snow_point_upper=0.25, brightness_coeff=1.2)\n",
    "random_rain_configs = [\n",
    "    RandomRain(p=1.0, slant_lower=-5, slant_upper=5, drop_length=20, drop_width=1, rain_type=\"drizzle\"),\n",
    "    RandomRain(p=1.0, slant_lower=0, slant_upper=0, drop_length=5, drop_width=5, drop_color=(200, 200, 220), blur_value=7, brightness_coefficient=0.77)\n",
    "]\n",
    "\n",
    "# Distortion augmentation function\n",
    "def distortion_augment(image):\n",
    "    augmentations = [\n",
    "        lambda x: fog_configs[0](image=x)[\"image\"],\n",
    "        lambda x: random_rain_configs[0](image=x)[\"image\"],\n",
    "        lambda x: random_snow(image=x)[\"image\"],\n",
    "        lambda x: splatter_configs[0](image=x)[\"image\"],\n",
    "        lambda x: gauss_noise(image=x)[\"image\"],\n",
    "        lambda x: clahe(image=x)[\"image\"],\n",
    "        lambda x: coarse_dropout(image=x)[\"image\"],\n",
    "        lambda x: pixel_dropout(image=x)[\"image\"],\n",
    "        lambda x: splatter_configs[1](image=x)[\"image\"],\n",
    "        lambda x: fog_configs[1](image=x)[\"image\"],\n",
    "        lambda x: splatter_configs[2](image=x)[\"image\"],\n",
    "        lambda x: random_rain_configs[1](image=x)[\"image\"]\n",
    "    ]\n",
    "    return random.choice(augmentations)(image)\n",
    "\n",
    "# Color augmentation function\n",
    "def color_augment(image):\n",
    "    if random.choice([True, False]):\n",
    "        # RGB enhancement\n",
    "        pil_image = Image.fromarray(np.uint8(image))\n",
    "        enhancer = ImageEnhance.Color(pil_image)\n",
    "        enhanced_image = enhancer.enhance(2)\n",
    "        return np.asarray(enhanced_image, dtype=np.uint8)\n",
    "    else:\n",
    "        return hue_sat_val(image=image)[\"image\"]\n",
    "\n",
    "# Preprocess with augmentation\n",
    "def preprocess_input_augment(image):\n",
    "    \"\"\"Apply augmentations and normalize the image.\"\"\"\n",
    "    image = np.uint8(image)\n",
    "    \n",
    "    # Always apply rotation\n",
    "    image = rotate_aug(image=image)[\"image\"]\n",
    "\n",
    "    # Apply random augmentations\n",
    "    if random.random() < 0.4:\n",
    "        pass  # Do nothing\n",
    "    elif random.random() < 0.6:\n",
    "        image = color_augment(image)\n",
    "    else:\n",
    "        image = distortion_augment(image)\n",
    "\n",
    "    # Normalize the image\n",
    "    image = (image / 255.0 - 0.5) * 2.0\n",
    "    return image\n",
    "\n",
    "# Preprocess without augmentation\n",
    "def preprocess_input(image):\n",
    "    \"\"\"Normalize the image without augmentation.\"\"\"\n",
    "    return (image / 255.0 - 0.5) * 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, prefix):\n",
    "    \"\"\"\n",
    "    Save the model's weights and architecture to files.\n",
    "\n",
    "    Args:\n",
    "        model (keras.Model): The model to save.\n",
    "        prefix (str): The file prefix for saving the model.\n",
    "    \"\"\"\n",
    "    # Save the model's weights\n",
    "    weights_path = f\"{prefix}.h5\"\n",
    "    model.save_weights(weights_path)\n",
    "    print(f\"Model weights saved to: {weights_path}\")\n",
    "\n",
    "    # Save the model's architecture in JSON format\n",
    "    architecture_path = f\"{prefix}.json\"\n",
    "    with open(architecture_path, \"w\") as json_file:\n",
    "        json_file.write(model.to_json())\n",
    "    print(f\"Model architecture saved to: {architecture_path}\")\n",
    "\n",
    "\n",
    "def count_files_in_folder(folder_path):\n",
    "    \"\"\"\n",
    "    Count the total number of files in a folder and its subfolders.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): The path to the folder.\n",
    "\n",
    "    Returns:\n",
    "        int: The total number of files.\n",
    "    \"\"\"\n",
    "    return sum(len(files) for _, _, files in os.walk(folder_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters\n",
    "batch_size = 16  # Number of images per batch (low due to high image size: 384x384px)\n",
    "nr_epochs = 1000\n",
    "img_width, img_height = 384, 384\n",
    "input_shape = (img_width, img_height, 3)\n",
    "\n",
    "# Model and dataset details\n",
    "model_name = \"inception_v3\"\n",
    "train_data_dir = \"/trainset/\"\n",
    "validation_data_dir = \"/valset\"\n",
    "nr_classes = 2\n",
    "tags = [\"positive\", \"negative\"]\n",
    "\n",
    "# Count the number of training and validation samples\n",
    "nr_train_samples = count_files_in_folder(train_data_dir)\n",
    "nr_val_samples = count_files_in_folder(validation_data_dir)\n",
    "\n",
    "# Dataset information\n",
    "print(\"Loading dataset:\")\n",
    "print(f\"Number of training samples: {nr_train_samples}\")\n",
    "print(f\"Number of validation samples: {nr_val_samples}\")\n",
    "\n",
    "# Set image data format\n",
    "K.set_image_data_format(\"channels_last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input_augment,\n",
    "    brightness_range=[0.97, 1.03],\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True\n",
    ")\n",
    "\n",
    "# Data preprocessing for validation\n",
    "validation_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "# Training data generator\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    directory=train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"binary\",\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Validation data generator\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    directory=validation_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"binary\",\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_inception_model(input_shape, num_classes=2):\n",
    "    \"\"\"\n",
    "    Builds and compiles an InceptionV3 model with a custom top layer.\n",
    "\n",
    "    Args:\n",
    "        input_shape (tuple): The shape of the input images (height, width, channels).\n",
    "        num_classes (int): Number of output classes (default: 2).\n",
    "\n",
    "    Returns:\n",
    "        keras.Model: The compiled model.\n",
    "    \"\"\"\n",
    "    # Initialize the base InceptionV3 model\n",
    "    base_model = InceptionV3(weights=None, input_shape=input_shape, include_top=False)\n",
    "    base_model.trainable = True\n",
    "\n",
    "    # Add the custom top layers\n",
    "    head_model = base_model.output\n",
    "    head_model = GlobalAveragePooling2D()(head_model)\n",
    "    head_model = Dropout(0.3)(head_model)\n",
    "    head_model = Dense(64, activation=\"relu\", kernel_regularizer=regularizers.l2(0.01))(head_model)\n",
    "    head_model = Dropout(0.3)(head_model)\n",
    "    output_layer = Dense(num_classes - 1, activation=\"sigmoid\")(head_model)\n",
    "\n",
    "    # Build the model\n",
    "    model = Model(inputs=base_model.input, outputs=output_layer)\n",
    "\n",
    "    # Ensure all layers are trainable\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = True\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "print(\"Loading original Inception model...\")\n",
    "model = build_inception_model(input_shape, num_classes=2)\n",
    "print(\"New model built successfully.\")\n",
    "\n",
    "# Uncomment to load pre-trained weights\n",
    "# weights_name = 'model_inceptionv3_cyto_v16_5_eberhard_v3.15.h5'\n",
    "# model.load_weights(weights_name)\n",
    "# print(f\"Weights loaded: {weights_name}\")\n",
    "\n",
    "# Define optimizer and compile the model\n",
    "print(\"Compiling the model...\")\n",
    "optimizer = SGD(learning_rate=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\n",
    "        \"binary_accuracy\",\n",
    "        AUC(),\n",
    "        Precision(),\n",
    "        Recall(),\n",
    "        TruePositives(),\n",
    "        TrueNegatives(),\n",
    "        FalsePositives(),\n",
    "        FalseNegatives(),\n",
    "    ]\n",
    ")\n",
    "print(\"Model compiled successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model weights\n",
    "weights_path = f\"{model_name}.h5\"\n",
    "model.save_weights(weights_path)\n",
    "print(f\"Model weights saved to {weights_path}\")\n",
    "\n",
    "# Serialize model architecture to JSON\n",
    "json_path = f\"{model_name}.json\"\n",
    "model_json = model.to_json()\n",
    "with open(json_path, \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "print(f\"Model architecture saved to {json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model checkpoint callback to save model weights after every epoch\n",
    "checkpoint_filepath = f\"{model_name}_epoch.{{epoch:02d}}.h5\"\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    save_freq='epoch',\n",
    "    save_best_only=False\n",
    ")\n",
    "print(f\"Model checkpoints will be saved to: {checkpoint_filepath}\")\n",
    "\n",
    "# CSV Logger callback to log training history\n",
    "csv_logger_filepath = f\"{model_name}_model_history_log.csv\"\n",
    "csv_logger = CSVLogger(csv_logger_filepath, append=True)\n",
    "print(f\"Training history will be logged to: {csv_logger_filepath}\")\n",
    "\n",
    "# List of callbacks\n",
    "callbacks_list = [model_checkpoint_callback, csv_logger]\n",
    "\n",
    "##################\n",
    "### Training #####\n",
    "##################\n",
    "\n",
    "print(\"Start training\")\n",
    "history = model.fit(\n",
    "    x=train_generator,\n",
    "    epochs=nr_epochs,\n",
    "    validation_data=validation_generator,\n",
    "    steps_per_epoch=nr_train_samples // batch_size,\n",
    "    validation_steps=nr_val_samples // batch_size,\n",
    "    callbacks=callbacks_list,\n",
    "    workers=8,\n",
    "    max_queue_size=16,\n",
    "    use_multiprocessing=True\n",
    ")\n",
    "\n",
    "##############################\n",
    "### Save Model and Weights ###\n",
    "##############################\n",
    "\n",
    "# Save model weights\n",
    "weights_filepath = f\"{model_name}.h5\"\n",
    "model.save_weights(weights_filepath)\n",
    "print(f\"Model weights saved to: {weights_filepath}\")\n",
    "\n",
    "# Serialize model architecture to JSON\n",
    "model_json_filepath = f\"{model_name}.json\"\n",
    "model_json = model.to_json()\n",
    "with open(model_json_filepath, \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "print(f\"Model architecture saved to: {model_json_filepath}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
